# import os
# from pycocotools.coco import COCO
# from pycocoevalcap.eval import COCOEvalCap

# def coco_caption_eval(annotation_file, results_file):
#     # ë””ì½”ë”ì—ì„œ ìƒì„±ëœ ê²°ê³¼ íŒŒì¼ì„ í‰ê°€í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
#     print("[Debug] evaltools/ic/__init__.py -> coco_caption_eval()í•¨ìˆ˜ í˜¸ì¶œ : generate ê²°ê³¼ í‰ê°€")
#     assert os.path.exists(annotation_file)

#     # create coco object and coco_result object
#     coco = COCO(annotation_file)
#     coco_result = coco.loadRes(results_file)

#     # create coco_eval object by taking coco and coco_result
#     coco_eval = COCOEvalCap(coco, coco_result)

#     # evaluate results
#     coco_eval.evaluate() #í‰ê°€ ê°ì²´ ë°˜í™˜

#     # print output evaluation scores
#     for metric, score in coco_eval.eval.items():
#         print(f'{metric}: {score:.3f}', flush=True)

#     return coco_eval

# --------------------
# Failure Case ìˆ˜ì§‘ìš© í•¨ìˆ˜ (revised)
# --------------------
import os, json, numpy as np
from pathlib import Path
from pycocotools.coco         import COCO
from pycocoevalcap.eval       import COCOEvalCap


def coco_caption_eval(
    annotation_file,
    results_file,
    dataset_name="rsicd",
    mask_name="mask_90",
    base_out="results/failure_case/captioning",
    q=10,                               # ðŸ”„ í•˜ìœ„ q% (0 ì œì™¸) ê¸°ì¤€
    metrics=None                        # ðŸ”„ í•„í„°ë§í•  ì§€í‘œ ì§€ì • ê°€ëŠ¥
):
    """
    RSICD ìº¡ì…˜ í‰ê°€ + failure case JSON ì €ìž¥
    - 0 ì ì€ ê¸°ë³¸ ì‹¤íŒ¨ë¡œ ê°„ì£¼, 0ì„ ì œì™¸í•œ í•˜ìœ„ q%ì—ë„ í¬í•¨
    - metric ì—¬ëŸ¬ ê°œë¥¼ ë„£ìœ¼ë©´ OR ì¡°ê±´(í•©ì§‘í•©)ìœ¼ë¡œ ìˆ˜ì§‘
    """
    assert os.path.exists(annotation_file), f"{annotation_file} not found"
    assert os.path.exists(results_file),    f"{results_file} not found"

    # 1) í‰ê°€ ---------------------------------------------------------------
    coco, coco_res = COCO(annotation_file), None
    coco_res = coco.loadRes(results_file)
    coco_eval = COCOEvalCap(coco, coco_res)
    coco_eval.evaluate()

    for m, sc in coco_eval.eval.items():
        print(f"{m}: {sc:.3f}")

    per_img = coco_eval.evalImgs
    if per_img is None:
        print("[Warning] no evalImgs â†’ cannot extract failure cases")
        return coco_eval

    if metrics is None:
        metrics = list(coco_eval.eval.keys())

    # 2) per-metric ì ìˆ˜ ë°°ì—´ + threshold ------------------------------------
    def score_of(info, metric):
        v = info.get(metric, 0.0)
        if isinstance(v, dict):                    # SPICE ë“±
            v = v.get("All", v.get("all", {}))
            if isinstance(v, dict):
                v = v.get("f", 0.0)
        try:
            return float(v)
        except (TypeError, ValueError):
            return 0.0

    scores = {m: np.array([score_of(e, m) for e in per_img]) for m in metrics}

    def nz_percentile(arr, q):
        nz = arr[arr > 0]
        return 0.0 if nz.size == 0 else float(np.percentile(nz, q))

    thresholds = {m: nz_percentile(v, q) for m, v in scores.items()}
    print("\n[Thresholds (0 ì œì™¸, {}-ptl)]".format(q))
    for m, t in thresholds.items():
        print(f"  {m:<8}: {t:.6g}")

    # 3) ìº¡ì…˜ lookup --------------------------------------------------------
    with open(results_file, "r", encoding="utf-8") as f:
        preds = json.load(f)
    gen_list = preds["annotations"] if isinstance(preds, dict) and "annotations" in preds else preds
    gen_caps = {d["image_id"]: d["caption"] for d in gen_list}

    with open(annotation_file, "r", encoding="utf-8") as f:
        gt_caps = {}
        for a in json.load(f)["annotations"]:
            gt_caps.setdefault(a["image_id"], []).append(a["caption"])

    # 4) ì‹¤íŒ¨ ì—”íŠ¸ë¦¬ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°) ----------------------------------------
    fail_dict = {}                                     # ðŸ”„ image_id â†’ info
    for info in per_img:
        iid = info["image_id"]
        for m in metrics:
            sc = score_of(info, m)
            if sc == 0 or sc <= thresholds[m]:         # 0 ì  or í•˜ìœ„ q%
                fd = fail_dict.setdefault(iid, {
                    "image_id":    iid,
                    "failed_metrics": [],
                    "scores":      {},
                    "gen_caption": gen_caps.get(iid, ""),
                    "gt_captions": gt_caps.get(iid, [])
                })
                fd["failed_metrics"].append(m)
                fd["scores"][m] = sc

    fail_list = list(fail_dict.values())
    print(f"\n[Fail-Dump] {len(fail_list)} entries")

    # 5) ì €ìž¥ ---------------------------------------------------------------
    out_dir  = Path(base_out) / dataset_name
    out_dir.mkdir(parents=True, exist_ok=True)
    out_f    = out_dir / f"{dataset_name}_failcases_{mask_name}.json"
    with open(out_f, "w", encoding="utf-8") as fp:
        json.dump(fail_list, fp, ensure_ascii=False, indent=2)
    print(f"[Saved] {out_f.resolve()}")
    return coco_eval
